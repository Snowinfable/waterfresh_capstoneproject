{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c32fd61",
   "metadata": {},
   "source": [
    "# Neural Network Training - Water Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57d8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351227a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>well_depth</th>\n",
       "      <th>TP-TotPop_mean</th>\n",
       "      <th>TP-TotPop_slope</th>\n",
       "      <th>PS-WGWFr_mean</th>\n",
       "      <th>PS-WGWFr_slope</th>\n",
       "      <th>PS-WGWSa_mean</th>\n",
       "      <th>PS-WGWSa_slope</th>\n",
       "      <th>PS-WSWFr_mean</th>\n",
       "      <th>PS-WSWFr_slope</th>\n",
       "      <th>...</th>\n",
       "      <th>apr_slope_ET</th>\n",
       "      <th>may_slope_ET</th>\n",
       "      <th>jun_slope_ET</th>\n",
       "      <th>jul_slope_ET</th>\n",
       "      <th>aug_slope_ET</th>\n",
       "      <th>sep_slope_ET</th>\n",
       "      <th>oct_slope_ET</th>\n",
       "      <th>nov_slope_ET</th>\n",
       "      <th>dec_slope_ET</th>\n",
       "      <th>confined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.698974</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>63.37225</td>\n",
       "      <td>6.4143</td>\n",
       "      <td>5.7275</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010231</td>\n",
       "      <td>0.018840</td>\n",
       "      <td>0.079734</td>\n",
       "      <td>0.078744</td>\n",
       "      <td>0.100108</td>\n",
       "      <td>0.019087</td>\n",
       "      <td>0.035189</td>\n",
       "      <td>0.013260</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>50.65425</td>\n",
       "      <td>-0.6701</td>\n",
       "      <td>8.1750</td>\n",
       "      <td>-1.162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007168</td>\n",
       "      <td>0.020043</td>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.070892</td>\n",
       "      <td>0.046095</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>0.042004</td>\n",
       "      <td>0.010613</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.070000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>14.17650</td>\n",
       "      <td>-0.2138</td>\n",
       "      <td>1.1850</td>\n",
       "      <td>-0.316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025363</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.079713</td>\n",
       "      <td>0.072048</td>\n",
       "      <td>-0.006652</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>19.56875</td>\n",
       "      <td>-0.8005</td>\n",
       "      <td>4.2225</td>\n",
       "      <td>-1.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.084853</td>\n",
       "      <td>0.127095</td>\n",
       "      <td>0.058579</td>\n",
       "      <td>-0.031923</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.039395</td>\n",
       "      <td>0.030780</td>\n",
       "      <td>0.017864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>19.56875</td>\n",
       "      <td>-0.8005</td>\n",
       "      <td>4.2225</td>\n",
       "      <td>-1.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016237</td>\n",
       "      <td>0.011932</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.036504</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.034573</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21444</th>\n",
       "      <td>0.669231</td>\n",
       "      <td>465.000000</td>\n",
       "      <td>215.69150</td>\n",
       "      <td>7.2538</td>\n",
       "      <td>21.2975</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.1300</td>\n",
       "      <td>-4.080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023367</td>\n",
       "      <td>-0.063918</td>\n",
       "      <td>-0.060792</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>-0.049348</td>\n",
       "      <td>-0.045055</td>\n",
       "      <td>-0.024361</td>\n",
       "      <td>-0.010512</td>\n",
       "      <td>-0.003227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21445</th>\n",
       "      <td>1.017857</td>\n",
       "      <td>299.638938</td>\n",
       "      <td>895.57625</td>\n",
       "      <td>57.9219</td>\n",
       "      <td>131.3025</td>\n",
       "      <td>-11.795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.5325</td>\n",
       "      <td>-21.743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027429</td>\n",
       "      <td>-0.086441</td>\n",
       "      <td>-0.112566</td>\n",
       "      <td>-0.119920</td>\n",
       "      <td>-0.102341</td>\n",
       "      <td>-0.073428</td>\n",
       "      <td>-0.031203</td>\n",
       "      <td>-0.006618</td>\n",
       "      <td>-0.008993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21446</th>\n",
       "      <td>1.344156</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>895.57625</td>\n",
       "      <td>57.9219</td>\n",
       "      <td>131.3025</td>\n",
       "      <td>-11.795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.5325</td>\n",
       "      <td>-21.743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>-0.084290</td>\n",
       "      <td>-0.111530</td>\n",
       "      <td>-0.119165</td>\n",
       "      <td>-0.101484</td>\n",
       "      <td>-0.072290</td>\n",
       "      <td>-0.031112</td>\n",
       "      <td>-0.006723</td>\n",
       "      <td>-0.008804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21447</th>\n",
       "      <td>2.342857</td>\n",
       "      <td>299.638938</td>\n",
       "      <td>244.12600</td>\n",
       "      <td>18.7802</td>\n",
       "      <td>38.3175</td>\n",
       "      <td>3.945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0125</td>\n",
       "      <td>-9.039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.056658</td>\n",
       "      <td>0.012609</td>\n",
       "      <td>0.023476</td>\n",
       "      <td>0.040032</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.008501</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21448</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>244.12600</td>\n",
       "      <td>18.7802</td>\n",
       "      <td>38.3175</td>\n",
       "      <td>3.945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0125</td>\n",
       "      <td>-9.039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032608</td>\n",
       "      <td>-0.083369</td>\n",
       "      <td>-0.094194</td>\n",
       "      <td>-0.097032</td>\n",
       "      <td>-0.065941</td>\n",
       "      <td>-0.061323</td>\n",
       "      <td>-0.027757</td>\n",
       "      <td>-0.007099</td>\n",
       "      <td>-0.005189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21389 rows Ã— 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          slope  well_depth  TP-TotPop_mean  TP-TotPop_slope  PS-WGWFr_mean  \\\n",
       "0      0.698974  319.000000        63.37225           6.4143         5.7275   \n",
       "1      0.000000  110.000000        50.65425          -0.6701         8.1750   \n",
       "2     -0.070000   97.000000        14.17650          -0.2138         1.1850   \n",
       "3      0.234000  120.000000        19.56875          -0.8005         4.2225   \n",
       "4      0.041000  160.000000        19.56875          -0.8005         4.2225   \n",
       "...         ...         ...             ...              ...            ...   \n",
       "21444  0.669231  465.000000       215.69150           7.2538        21.2975   \n",
       "21445  1.017857  299.638938       895.57625          57.9219       131.3025   \n",
       "21446  1.344156  120.000000       895.57625          57.9219       131.3025   \n",
       "21447  2.342857  299.638938       244.12600          18.7802        38.3175   \n",
       "21448  0.016667  138.000000       244.12600          18.7802        38.3175   \n",
       "\n",
       "       PS-WGWFr_slope  PS-WGWSa_mean  PS-WGWSa_slope  PS-WSWFr_mean  \\\n",
       "0               0.939            0.0             0.0         0.2500   \n",
       "1              -1.162            0.0             0.0         0.0000   \n",
       "2              -0.316            0.0             0.0         0.0000   \n",
       "3              -1.223            0.0             0.0         0.0000   \n",
       "4              -1.223            0.0             0.0         0.0000   \n",
       "...               ...            ...             ...            ...   \n",
       "21444          -0.675            0.0             0.0        18.1300   \n",
       "21445         -11.795            0.0             0.0       140.5325   \n",
       "21446         -11.795            0.0             0.0       140.5325   \n",
       "21447           3.945            0.0             0.0        26.0125   \n",
       "21448           3.945            0.0             0.0        26.0125   \n",
       "\n",
       "       PS-WSWFr_slope  ...  apr_slope_ET  may_slope_ET  jun_slope_ET  \\\n",
       "0              -0.300  ...     -0.010231      0.018840      0.079734   \n",
       "1               0.000  ...     -0.007168      0.020043      0.050329   \n",
       "2               0.000  ...     -0.025363      0.010979      0.079713   \n",
       "3               0.000  ...      0.000335      0.084853      0.127095   \n",
       "4               0.000  ...     -0.016237      0.011932      0.026365   \n",
       "...               ...  ...           ...           ...           ...   \n",
       "21444          -4.080  ...     -0.023367     -0.063918     -0.060792   \n",
       "21445         -21.743  ...     -0.027429     -0.086441     -0.112566   \n",
       "21446         -21.743  ...     -0.025894     -0.084290     -0.111530   \n",
       "21447          -9.039  ...      0.030956      0.022669      0.056658   \n",
       "21448          -9.039  ...     -0.032608     -0.083369     -0.094194   \n",
       "\n",
       "       jul_slope_ET  aug_slope_ET  sep_slope_ET  oct_slope_ET  nov_slope_ET  \\\n",
       "0          0.078744      0.100108      0.019087      0.035189      0.013260   \n",
       "1          0.070892      0.046095      0.034398      0.042004      0.010613   \n",
       "2          0.072048     -0.006652      0.028606      0.026256      0.012102   \n",
       "3          0.058579     -0.031923      0.055627      0.039395      0.030780   \n",
       "4          0.036504      0.055363      0.005465      0.034573      0.001122   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "21444     -0.059517     -0.049348     -0.045055     -0.024361     -0.010512   \n",
       "21445     -0.119920     -0.102341     -0.073428     -0.031203     -0.006618   \n",
       "21446     -0.119165     -0.101484     -0.072290     -0.031112     -0.006723   \n",
       "21447      0.012609      0.023476      0.040032      0.015884      0.008501   \n",
       "21448     -0.097032     -0.065941     -0.061323     -0.027757     -0.007099   \n",
       "\n",
       "       dec_slope_ET  confined  \n",
       "0          0.005261         1  \n",
       "1          0.007278         0  \n",
       "2          0.006455         0  \n",
       "3          0.017864         0  \n",
       "4          0.004548         0  \n",
       "...             ...       ...  \n",
       "21444     -0.003227         0  \n",
       "21445     -0.008993         0  \n",
       "21446     -0.008804         0  \n",
       "21447      0.007605         0  \n",
       "21448     -0.005189         0  \n",
       "\n",
       "[21389 rows x 247 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = pd.read_csv('final_without_totals.csv')\n",
    "base_df['aquifer_type_id'] = base_df['aquifer_type_id'].where(base_df['aquifer_type_id'] == 'confined', 0)\n",
    "base_df['confined'] = base_df['aquifer_type_id'].where(base_df['aquifer_type_id'] == 0, 1)\n",
    "base_df.drop(['aquifer_type_id', 'decadal_change', 'county_fips'], axis =1, inplace = True)\n",
    "base_df = base_df.dropna()\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca82917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split\n",
    "X = base_df.drop(['slope'], axis = 1)#.to_numpy()\n",
    "y = np.array(base_df['slope'])\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faeb4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data = X_train.drop(['confined'], axis = 1)\n",
    "scaler.fit(data)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train.drop(['confined'], axis = 1))\n",
    "X_train = np.concatenate([X_train_scaled, np.array(list(X_train['confined'])).reshape(len(X_train), 1)], axis = 1)\n",
    "\n",
    "X_valid_scaled = scaler.transform(X_valid.drop(['confined'], axis = 1))\n",
    "X_valid = np.concatenate([X_valid_scaled, np.array(list(X_valid['confined'])).reshape(len(X_valid), 1)], axis = 1)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test.drop(['confined'], axis =1))\n",
    "X_test = np.concatenate([X_test_scaled, np.array(list(X_test['confined'])).reshape(len(X_test), 1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c960ba",
   "metadata": {},
   "source": [
    "# set up the model and data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e7ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.as_tensor(X_train, dtype = torch.float)\n",
    "y_train_tensor = torch.as_tensor(y_train, dtype = torch.float)\n",
    "X_valid_tensor = torch.as_tensor(X_valid, dtype = torch.float)\n",
    "y_valid_tensor = torch.as_tensor(y_valid, dtype = torch.float)\n",
    "X_test_tensor = torch.as_tensor(X_test, dtype = torch.float)\n",
    "y_test_tensor = torch.as_tensor(y_test, dtype = torch.float)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16956b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, hidden1, hidden2, hidden3, hidden4, drop):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=len(base_df.columns)-1, out_features=hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_features=hidden1, out_features=hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_features=hidden2, out_features=hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_features=hidden3, out_features=hidden4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_features=hidden4, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415f1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop for training MLP through gradient descent\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "\n",
    "    model.train() # Set your model to \"train\" mode\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        # Send inputs and labels to device\n",
    "        y=y.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss. reshape predictions\n",
    "        pred = model(X).reshape(-1) \n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9b83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If evaluating model training while it trains, include function below with train loop\n",
    "def test_loop(dataloader, model, loss_fn, device, count): \n",
    "    total = len(dataloader.dataset)\n",
    "    test_loss = 0\n",
    "\n",
    "    model.eval() # Set model to \"eval\" mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Send inputs and labels to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X).reshape(-1) # Reshape the pred of shape (N, 1) to (N,)\n",
    "            loss = loss_fn(pred, y)\n",
    "            #no optimization \n",
    "\n",
    "            test_loss += loss.item() * X.size(0) # because \"loss\" was averaged over the batch\n",
    "\n",
    "    test_loss /= total\n",
    "    \n",
    "    #uncomment below to track test loss\n",
    "    if count % 30 == 0:\n",
    "        print(f\"Avg test loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123de0a6-7688-4a52-834c-5aacf8e81855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3683328",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "losses = [(nn.functional.mse_loss, 'mse')]\n",
    "\n",
    "\n",
    "#hyperparameters. Edit these as needed\n",
    "dropouts = [.15]\n",
    "lrs = [5e-3, 5e-2, 1e-1]\n",
    "epoch_size = [300, 450]\n",
    "batch_size = [100, 300, 500,700]\n",
    "hidden_sizes = [[150,100, 50, 25], [200,100, 50, 25], [150,100,50, 10], [200, 100,20,5]]\n",
    "\n",
    "\n",
    "error_vals = []\n",
    "lr_vals = []\n",
    "batches = []\n",
    "epochs = []\n",
    "loss_functions = []\n",
    "drop_vals = []\n",
    "hidden_vals = []\n",
    "\n",
    "for loss_fn in losses:\n",
    "    for hiddens in hidden_sizes:\n",
    "        for ep in epoch_size:\n",
    "            for bs in batch_size:\n",
    "                for drop in dropouts:\n",
    "                    for lr_ in lrs:\n",
    "                        model = NN(hiddens[0], hiddens[1], hiddens[2], hiddens[3], drop).to(device)\n",
    "                        train_dataloader = DataLoader(train_dataset, batch_size = bs, shuffle = True)\n",
    "                        valid_dataloader = DataLoader(valid_dataset, batch_size = bs, shuffle = True)\n",
    "                        test_dataloader = DataLoader(test_dataset, batch_size = bs, shuffle = False)\n",
    "                        optimizer = optim.SGD(params = model.parameters(), lr = lr_)\n",
    "                        \n",
    "                        #keep track as model training runs\n",
    "                        display('h1: ' + str(hiddens[0]) + ' - h2: ' + str(hiddens[1]) + ' - h3: ' + str(hiddens[2]) + ' - h4: ' + str(hiddens[3]) + ' -- lr = ' + str(lr_) + ' -- ep = ' + str(ep) + ' -- bs = ' + str(bs))\n",
    "                        \n",
    "                        #train the model\n",
    "                        for t in range(ep):\n",
    "                            train_loop(train_dataloader, model, loss_fn[0], optimizer, device)\n",
    "                            test_loop(valid_dataloader, model, loss_fn[0], device, count)\n",
    "                            count +=1\n",
    "        \n",
    "                        #make predictions and pull truth values\n",
    "                        final_loss = test_loop(test_dataloader, model, loss_fn[0], device, count)\n",
    "                        display('final loss: ' + str(final_loss))\n",
    "                        \n",
    "                        #Add values to lists, which will then populate the hyper-parameter and evaluate metric dataframe\n",
    "                        error_vals.append(final_loss)\n",
    "                        lr_vals.append(lr_)\n",
    "                        batches.append(bs)\n",
    "                        epochs.append(ep)\n",
    "                        loss_functions.append(loss_fn[1])\n",
    "                        drop_vals.append(drop)\n",
    "                        hidden_vals.append('h1: ' + str(hiddens[0]) + ' - h2: ' + str(hiddens[1]) + ' - h3: ' + str(hiddens[2]) + ' - h4: ' + str(hiddens[3]))\n",
    "\n",
    "#Output Dataframe\n",
    "output_df = pd.DataFrame({'hiddens': hidden_vals, 'batch size': batches, 'epochs':epochs, \n",
    "                          'lr': lr_vals, 'loss func':loss_functions , 'dropout': drop_vals, 'test_mse':error_vals})\n",
    "output_df['data in'] = 'w/out totals, w aqtype, st scaler, slope'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
