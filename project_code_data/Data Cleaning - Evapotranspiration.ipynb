{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674abd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from shapely.geometry import Point\n",
    "from rasterio.warp import transform\n",
    "from pyproj import CRS\n",
    "from pyproj import Transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafb68d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trends = pd.read_csv('merge_data/trends_clean.csv')\n",
    "trends_filtered = trends[['site_id','latitude', 'longitude']]\n",
    "\n",
    "#split into chunks for faster processing.\n",
    "trends_filtered_1 = trends_filtered.iloc[:3500,:].reset_index(drop = True)\n",
    "trends_filtered_2 = trends_filtered.iloc[3500:7000,:].reset_index(drop = True)\n",
    "trends_filtered_3 = trends_filtered.iloc[7000:10500,:].reset_index(drop = True)\n",
    "trends_filtered_4 = trends_filtered.iloc[10500:14000,:].reset_index(drop = True)\n",
    "trends_filtered_5 = trends_filtered.iloc[14000:18000,:].reset_index(drop = True)\n",
    "trends_filtered_6 = trends_filtered.iloc[18000:,:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174d332",
   "metadata": {},
   "source": [
    "## Process all geotifs to read the evapostranspiration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337a1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tifs(df):\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    years = [str(x) for x in range(2000,2019)]\n",
    "    all_dicts = {}\n",
    "    for site in df['site_id']:\n",
    "        all_dicts[site] = {'site':[site]*19, 'year':years}\n",
    "        for month in months:\n",
    "            all_dicts[site][month] = []\n",
    "    \n",
    "    for year in years:\n",
    "        print('******')\n",
    "        print('******')\n",
    "        print('******')\n",
    "        print(year)\n",
    "        print('------')\n",
    "        month_vals = []                \n",
    "        for month in months:\n",
    "            print(month)\n",
    "            \n",
    "            year_month = year + '_' + month\n",
    "            column = 'AET_' + year_month\n",
    "            \n",
    "            if column == 'AET_2018_10':\n",
    "                for m in ['10', '11', '12']:\n",
    "                    for i, row in df.iterrows():\n",
    "                        all_dicts[row['site_id']][m].append(None)\n",
    "                break\n",
    "\n",
    "            file_path = 'monthly_ets/AET_' + year_month + '.tif'\n",
    "\n",
    "            count = 0\n",
    "            with rasterio.open(file_path) as src:\n",
    "                #set source and target coordinate systems\n",
    "                src_crs = CRS(\"WGS84\")\n",
    "                dst_crs = CRS(\"NAD83\")\n",
    "                \n",
    "                #pull the ET data from the geotifs for the site latitude and longitude\n",
    "                for i, row in df.iterrows():\n",
    "                    lat = row['latitude']\n",
    "                    lon = row['longitude']\n",
    "                    transformer = Transformer.from_crs(src_crs, dst_crs)\n",
    "                    dst_lon, dst_lat = transformer.transform(lon, lat)\n",
    "                    value = next(src.sample([(dst_lon, dst_lat)]))[0]\n",
    "                    all_dicts[row['site_id']][month].append(value)\n",
    "\n",
    "    return all_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a057af1",
   "metadata": {},
   "source": [
    " Need to run below for each chunk, change output csv name each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b37f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = read_tifs(trends_filtered_1) #change for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045450d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for dictionary in full.values():\n",
    "    df = pd.DataFrame.from_dict(dictionary, orient = 'columns')\n",
    "    all_dfs.append(df)\n",
    "out_df = pd.concat(all_dfs).reset_index(drop = True)\n",
    "\n",
    "out_df.to_csv('ET_chunk1.csv', index = False) #change export name for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9a244",
   "metadata": {},
   "source": [
    "#### Can start from here if all processing of he geotifs has been completed: Merge chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e5ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = pd.read_csv('ET_chunk1.csv')\n",
    "chunk2 = pd.read_csv('ET_chunk2.csv')\n",
    "chunk3 = pd.read_csv('ET_chunk3.csv')\n",
    "chunk4 = pd.read_csv('ET_chunk4.csv')\n",
    "chunk5 = pd.read_csv('ET_chunk5.csv')\n",
    "chunk6 = pd.read_csv('ET_chunk6.csv')\n",
    "\n",
    "et_df = pd.concat([chunk1, chunk2, chunk3, chunk4, chunk5, chunk6])\n",
    "et_df = et_df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e9a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_df = et_df.rename(columns = {'01':'jan', '02':'feb', '03':'mar', '04':'apr', '05':'may', '06':'jun', \n",
    "                                '07':'jul', '08':'aug', '09':'sep', '10':'oct', '11':'nov', '12':'dec'\n",
    "                               }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe205f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we replace zeros with np.nan, since zeros are actually missing values\n",
    "et_df.replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4412509",
   "metadata": {},
   "source": [
    "#### Calculate the means and slopes for each month for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04738953",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = {}\n",
    "column_months = ['jan_mean', 'feb_mean', 'mar_mean', 'apr_mean', 'may_mean', 'jun_mean', 'jul_mean', 'aug_mean', \n",
    "                 'sep_mean', 'oct_mean', 'nov_mean', 'dec_mean', 'jan_slope', 'feb_slope', 'mar_slope', 'apr_slope', \n",
    "                 'may_slope', 'jun_slope', 'jul_slope', 'aug_slope', 'sep_slope', 'oct_slope', 'nov_slope', 'dec_slope'\n",
    "                ]\n",
    "column_months_spec = [x + '_ET' for x in column_months]\n",
    "\n",
    "dropped = et_df.drop(['year'], axis = 1)\n",
    "grouped = dropped.groupby(['site'])\n",
    "means_df = grouped.mean()\n",
    "groups = grouped.groups.keys()\n",
    "\n",
    "et_dict = {}\n",
    "count =0\n",
    "for g in groups:\n",
    "    group_df = grouped.get_group(g).reset_index(drop = True)\n",
    "    count +=1\n",
    "    if count %100 == 0:\n",
    "        print(count)\n",
    "        \n",
    "    #pull mean data\n",
    "    month_means = list(means_df.loc[g])\n",
    "\n",
    "    #find slopes\n",
    "    month_slopes = []\n",
    "    for month in group_df.columns[1:]:\n",
    "        data = group_df[month].dropna()\n",
    "        if data.empty == True:\n",
    "            month_slopes.append(np.nan)\n",
    "            continue\n",
    "        x = np.array(list(range(len(data)))).reshape(-1,1)\n",
    "        y = np.array(data).reshape(-1,1)\n",
    "        linreg = LinearRegression().fit(x, y)            \n",
    "        slope = linreg.coef_[0][0]\n",
    "        month_slopes.append(slope)\n",
    "\n",
    "    #dict with list data for each site\n",
    "    full_data = [g] + month_means + month_slopes\n",
    "    et_dict[g] = full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "754a1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_cleaned = pd.DataFrame.from_dict(et_dict, orient = 'index')\n",
    "et_cleaned.columns = ['site_id'] + column_months_spec\n",
    "et_cleaned.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2a26f",
   "metadata": {},
   "source": [
    "Now we handle missing data. Lets take the mean value from the two surrounding months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c980b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = et_cleaned.columns\n",
    "for i, row in et_cleaned.iterrows():\n",
    "    count = 0\n",
    "    for col in cols[1:13]:\n",
    "        count+=1\n",
    "        if pd.isna(row[col]) == True:\n",
    "            if col == 'jan_mean':\n",
    "                newval_mean = (row['feb_mean'] + row['dec_mean'])/2\n",
    "                et_cleaned.loc[i, 'jan_mean'] = newval_mean\n",
    "                newval_slope = (row['feb_slope'] + row['dec_slope'])/2\n",
    "                et_cleaned.loc[i,'jan_mean'] = newval_slope\n",
    "                continue\n",
    "                \n",
    "            if col == 'dec_mean':\n",
    "                newval_mean = (row['nov_mean'] + row['jan_mean'])/2\n",
    "                et_cleaned.loc[i, 'dec_mean'] = newval_mean\n",
    "                newval_slope = (row['nov_slope'] + row['jan_slope'])/2\n",
    "                et_cleaned.loc[i,'dec_slope'] = newval_slope\n",
    "                continue\n",
    "                                \n",
    "            if pd.isna(row[cols[count-1]]) == True:\n",
    "                newval_mean = row[cols[count+1]]\n",
    "                newval_slope = row[cols[count+13]]\n",
    "                et_cleaned.loc[i, col] = newval_mean\n",
    "                et_cleaned.loc[i,cols[count +12]] = newval_slope\n",
    "                continue\n",
    "            if pd.isna(row[cols[count+1]]) == True:\n",
    "                newval_mean = row[cols[count-1]]\n",
    "                newval_slope = row[cols[count+11]]\n",
    "                et_cleaned.loc[i, col] = newval_mean\n",
    "                et_cleaned.loc[i,cols[count +12]] = newval_slope\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            newval_mean = (row[cols[count-1]] + row[cols[count+1]])/2\n",
    "            newval_slope = (row[cols[count+11]] + row[cols[count+13]])/2\n",
    "            et_cleaned.loc[i, col] = newval_mean\n",
    "            et_cleaned.loc[i,cols[count +12]] = newval_slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e888e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_cleaned.to_csv('merge_data/et_cleaned.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
