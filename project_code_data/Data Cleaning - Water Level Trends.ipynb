{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9727a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import addfips\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geocoding import geocode, reverse_geocode\n",
    "from arcgis.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67770668",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_stations = pd.read_csv('CA_stations_v2.0.csv')\n",
    "nwis = pd.read_csv('NWIS_gwl_meta_v2.0.csv')\n",
    "ngwmn = pd.read_csv('NGWMN_gwl_meta_v2.0.csv')\n",
    "\n",
    "trends=pd.read_csv('GW_trendsout_v2.0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ddf54",
   "metadata": {},
   "source": [
    "Using the ca_stations, nwis, and ngwmn datasets, pull all site information into dictionaries (lat, long, well_depth, aquifer code, gse-if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8d0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict = {}\n",
    "for i, row in ca_stations.iterrows():\n",
    "    all_dict[row['site_code']] = {'latitude': row['latitude'], 'longitude': row['longitude'], 'gse': row['gse'], 'well_depth':row['well_depth']}\n",
    "    \n",
    "for i,row in nwis.iterrows():\n",
    "    site_name = row['agency_cd'] + '.' + str(row['site_no'])\n",
    "    all_dict[site_name] = {'latitude': row['dec_lat_va'], 'longitude':row['dec_long_va'], 'well_depth':row['well_depth_va'], 'aqfr_code': row['nat_aqfr_cd']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1672418",
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for i,row in ngwmn.iterrows():\n",
    "    site_replace = row['MY_SITEID'].replace(':', '.') \n",
    "    all_dict[site_replace] = {'latitude': row['DEC_LAT_VA'], 'longitude': row['DEC_LONG_V'], 'well_depth': row['WELL_DEPTH'], 'aqfr_code':row['NAT_AQUIFE']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d73de",
   "metadata": {},
   "source": [
    "Create columns with this new information in the main 'trends' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cc83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends['latitude'] = None\n",
    "trends['longitude'] = None\n",
    "trends['well_depth'] = None\n",
    "trends['county_name'] = None\n",
    "trends['county_fips'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad0592",
   "metadata": {},
   "source": [
    "Add site infromation to each site in the trends dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "miss_list =[] #to see if there are sites in the trends dataset that were not in the ca_stations, nwis, or ngwmn datasets.\n",
    "\n",
    "for i, row in trends.iterrows():\n",
    "    count +=1\n",
    "    if count % 1000 ==0:\n",
    "        print(count)\n",
    "        \n",
    "    site_id = row['site_id']\n",
    "    try:\n",
    "        entry = all_dict[site_id]\n",
    "    except:\n",
    "        miss_list.append(site_id)\n",
    "        continue\n",
    "    \n",
    "    trends.loc[i, 'latitude'] = entry['latitude']\n",
    "    trends.loc[i, 'longitude'] = entry['longitude']\n",
    "    trends.loc[i, 'well_depth'] = entry['well_depth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d7113",
   "metadata": {},
   "source": [
    "Use the location data to add the county and county fips code to each site in the trends dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe240918",
   "metadata": {},
   "outputs": [],
   "source": [
    "gis = GIS()\n",
    "count = 0\n",
    "no_fips = []\n",
    "for i, row in trends.iterrows():\n",
    "    count +=1\n",
    "    if count % 1000 ==0:\n",
    "        print(count)\n",
    "    \n",
    "    if row['site_id'] in miss_list:\n",
    "        continue\n",
    "    \n",
    "    # Latitude and Longitude coordinates\n",
    "    latitude = trends.loc[i, 'latitude']\n",
    "    longitude = trends.loc[i, 'longitude'] \n",
    "    \n",
    "    #create a point object for ArcGIS\n",
    "    point = Point({'x': longitude, 'y': latitude})\n",
    "    result = reverse_geocode(point) \n",
    "    \n",
    "    #pull the county information and update the trends dataset\n",
    "    county = result['address']['Subregion']\n",
    "    st =  result['address']['Region']\n",
    "    af = addfips.AddFIPS()\n",
    "    fips = af.get_county_fips(county, state=st)\n",
    "    \n",
    "    trends.loc[i, 'county_name'] = county\n",
    "    trends.loc[i, 'county_fips'] = fips\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08edb1ae",
   "metadata": {},
   "source": [
    "## Filter trends -- get rid of duplicates, filter by year cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718e831",
   "metadata": {},
   "source": [
    "We can change how we to set the year cutoffs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa80b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the years\n",
    "trend_filt = trends[(abs(trends['trend_period_start_yr']-2000)<4) & (trends['trend_period_end_yr']>2016)]\n",
    "\n",
    "#we want to select the 'mean' water level as the data point of interest\n",
    "trend_filt = trend_filt[trend_filt['metric_id'] == 'mean']\n",
    "\n",
    "#if the site id, slope, decadal change, and start year are the exact same, we can filter to just include one of them\n",
    "trend_filt = trend_filt.drop_duplicates(subset = ['site_id', 'slope', 'decadal_change', 'trend_period_start_yr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719d98a",
   "metadata": {},
   "source": [
    "Filter multiple data entries for the same site. We want to the shortest time interval possible for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e733ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = trend_filt[trend_filt.duplicated(subset=['site_id']) == True]\n",
    "for site in dups['site_id'].unique():\n",
    "    ids = trend_filt.index[trend_filt['site_id'] == site].tolist() #find index vals in the trends dataframe for each site_id\n",
    "    \n",
    "    #find start years for the repeated site_id entries, and add to dictionary\n",
    "    comp_dict = {}\n",
    "    for i in ids:\n",
    "        start_year= trend_filt.loc[i, 'trend_period_start_yr']\n",
    "        comp_dict[i] = start_year\n",
    "    \n",
    "    #find the most recent start year for the site_id entries\n",
    "    maxval = 1\n",
    "    max_entry = None\n",
    "    for entry in comp_dict.keys():\n",
    "        if int(comp_dict[entry]) > maxval:\n",
    "            max_entry = entry\n",
    "            \n",
    "    #select and drop the duplicates we don't want        \n",
    "    drop_list = []\n",
    "    for entry in comp_dict:\n",
    "        if entry != max_entry:\n",
    "            drop_list.append(entry)\n",
    "    trend_filt.drop(drop_list,axis = 0,inplace = True)\n",
    "trend_filt.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436808d9",
   "metadata": {},
   "source": [
    "export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc751e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_filt.to_csv('merge_data/trends_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce9be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
